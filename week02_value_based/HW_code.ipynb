{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphviz available: True\n"
     ]
    }
   ],
   "source": [
    "from mdp import MDP\n",
    "import numpy as np\n",
    "\n",
    "from mdp import has_graphviz\n",
    "from mdp import plot_graph\n",
    "from mdp import FrozenLakeEnv\n",
    "from IPython.display import display\n",
    "print(\"Graphviz available:\", has_graphviz)\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration (VI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q_sa(mdp: MDP, V, s, a, gamma):\n",
    "    \"\"\"\n",
    "    Оценивает полезность действия a в состоянии s, если \n",
    "    полезности всех состояний - V.\n",
    "    \"\"\"\n",
    "    assert a in mdp.get_possible_actions(s)\n",
    "\n",
    "    Q = 0\n",
    "    for s_new, p in mdp.get_next_states(s, a).items():\n",
    "        r = mdp.get_reward(s, a, s_new)\n",
    "        V_new = V[s_new]\n",
    "        Q += p * (r + gamma * V_new)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_V_s(mdp, V, s, gamma):\n",
    "    \"\"\"\n",
    "    Вычисляет новую полезность состояния s в алгоритме VI,\n",
    "    если полезности всех состояний равны V.\n",
    "    \"\"\"\n",
    "    if mdp.is_terminal(s):\n",
    "        return 0\n",
    "\n",
    "    return np.max([\n",
    "        get_Q_sa(mdp, V, s, a, gamma) \n",
    "        for a in mdp.get_possible_actions(s)\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_new_V(mdp, V, gamma):\n",
    "    \"\"\"\n",
    "    Вычисляет новую полезность всех состояний в алгоритме VI,\n",
    "    если полезности всех состояний равны V.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        s: get_new_V_s(mdp, V, s, gamma)\n",
    "        for s in mdp.get_all_states()\n",
    "    }\n",
    "\n",
    "def get_V_diff(mdp, V1, V0):\n",
    "    return max(abs(V1[s] - V0[s]) for s in mdp.get_all_states())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "        mdp, gamma, V=None, \n",
    "        max_steps=1000, \n",
    "        min_diff=1e-5,\n",
    "        ):\n",
    "    \"\"\" \n",
    "    Делает num_iter шагов VI начиная с полезностей состояний V. \n",
    "    Прерывается, если последовательные V достаточно близки.\n",
    "    \"\"\"\n",
    "    V = V or {s: 0 for s in mdp.get_all_states()}\n",
    "    V_at_each_step = [V]\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        new_V = get_new_V(mdp, V, gamma)\n",
    "        V_at_each_step.append(new_V)\n",
    "\n",
    "        diff = get_V_diff(mdp, new_V, V)\n",
    "        V = new_V\n",
    "        \n",
    "        step += 1\n",
    "        if diff < min_diff:\n",
    "            break\n",
    "\n",
    "    return V, V_at_each_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, V, s, gamma):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(s):\n",
    "        return None\n",
    "\n",
    "    return max(\n",
    "        mdp.get_possible_actions(s), \n",
    "        key=lambda a: get_Q_sa(mdp, V, s, a, gamma)\n",
    "        )\n",
    "\n",
    "\n",
    "def get_optimal_policy(mdp: MDP, V, gamma):\n",
    "    return {\n",
    "        s: get_optimal_action(mdp, V, s, gamma) \n",
    "        for s in mdp.get_all_states()\n",
    "        }\n",
    "\n",
    "def are_policies_equal(policy1, policy2):\n",
    "    return set(policy1.items()) == set(policy2.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP для которого VI долго сходится (1 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vi_breaking_mdp(gamma, N):\n",
    "    \"\"\"\n",
    "    Возвращает MDP с 3 состояниями и 2 действиями,\n",
    "    для которого VI начинает выдавать правильную стратегию\n",
    "    лишь спустя N шагов, при дисконтировании награды gamma\n",
    "    Возвращает построенный MDP и функцию полезности состояний для него.\n",
    "    \"\"\"\n",
    "\n",
    "    transition_probs = {\n",
    "        's0': {\n",
    "            'a1': {'s1': 1},\n",
    "            'a2': {'s2': 1}\n",
    "        },\n",
    "        's1': {'a1': {'s1': 1}},\n",
    "        's2': {'a2': {'s2': 1}},\n",
    "    }\n",
    "\n",
    "    r = gamma*np.mean([1-gamma**(N-1), 1-gamma**N])/(1-gamma)\n",
    "    rewards = {\n",
    "        's0': {\n",
    "            'a1': {'s1': 0},\n",
    "            'a2': {'s2': r}\n",
    "        },\n",
    "        's1': {'a1': {'s1': 1}},\n",
    "        's2': {'a2': {'s2': 0}},\n",
    "    }\n",
    "\n",
    "    V = {'s0': gamma/(1-gamma), 's1': 1/(1-gamma), 's2': 0}\n",
    "    mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "    return mdp, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: MDP Pages: 1 -->\n",
       "<svg width=\"751pt\" height=\"186pt\"\n",
       " viewBox=\"0.00 0.00 751.31 186.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 182)\">\n",
       "<title>MDP</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-182 747.31,-182 747.31,4 -4,4\"/>\n",
       "<!-- s0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>s0</title>\n",
       "<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"40\" cy=\"-94\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"40\" cy=\"-94\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-87.8\" font-family=\"Arial\" font-size=\"24.00\">s0</text>\n",
       "</g>\n",
       "<!-- s0&#45;a1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>s0&#45;a1</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"144.58\" cy=\"-134\" rx=\"27.65\" ry=\"27.65\"/>\n",
       "<text text-anchor=\"middle\" x=\"144.58\" y=\"-129\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0&#45;a1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>s0&#45;&gt;s0&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M77.45,-108.22C87.75,-112.23 98.92,-116.59 109.1,-120.56\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"107.91,-123.85 118.5,-124.22 110.45,-117.33 107.91,-123.85\"/>\n",
       "</g>\n",
       "<!-- s0&#45;a2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>s0&#45;a2</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"144.58\" cy=\"-58\" rx=\"27.65\" ry=\"27.65\"/>\n",
       "<text text-anchor=\"middle\" x=\"144.58\" y=\"-53\" font-family=\"Arial\" font-size=\"20.00\">a2</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0&#45;a2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>s0&#45;&gt;s0&#45;a2</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M78.03,-81C88.02,-77.5 98.79,-73.72 108.65,-70.26\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"109.88,-73.54 118.16,-66.92 107.56,-66.93 109.88,-73.54\"/>\n",
       "</g>\n",
       "<!-- s1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>s1</title>\n",
       "<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"495.15\" cy=\"-138\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"495.15\" cy=\"-138\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"495.15\" y=\"-131.8\" font-family=\"Arial\" font-size=\"24.00\">s1</text>\n",
       "</g>\n",
       "<!-- s0&#45;a1&#45;&gt;s1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>s0&#45;a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M172.35,-134.31C230.74,-134.98 370.28,-136.58 444.98,-137.44\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"445.03,-140.94 455.07,-137.55 445.11,-133.94 445.03,-140.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.65\" y=\"-142.2\" font-family=\"Arial\" font-size=\"16.00\">p = 1</text>\n",
       "</g>\n",
       "<!-- s1&#45;a1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>s1&#45;a1</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"715.73\" cy=\"-134\" rx=\"27.65\" ry=\"27.65\"/>\n",
       "<text text-anchor=\"middle\" x=\"715.73\" y=\"-129\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s1&#45;a1 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>s1&#45;&gt;s1&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M535.23,-137.28C575.7,-136.54 638.44,-135.4 678.01,-134.67\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"678.1,-138.17 688.03,-134.49 677.97,-131.17 678.1,-138.17\"/>\n",
       "</g>\n",
       "<!-- s2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>s2</title>\n",
       "<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"495.15\" cy=\"-40\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"495.15\" cy=\"-40\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"495.15\" y=\"-33.8\" font-family=\"Arial\" font-size=\"24.00\">s2</text>\n",
       "</g>\n",
       "<!-- s0&#45;a2&#45;&gt;s2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>s0&#45;a2&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M172.35,-56.62C230.74,-53.6 370.28,-46.4 444.98,-42.54\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"445.27,-46.03 455.07,-42.02 444.9,-39.04 445.27,-46.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.65\" y=\"-60.2\" font-family=\"Arial\" font-size=\"16.00\">p = 1 &#160;reward =18.8784733475826</text>\n",
       "</g>\n",
       "<!-- s2&#45;a2 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>s2&#45;a2</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"715.73\" cy=\"-54\" rx=\"27.65\" ry=\"27.65\"/>\n",
       "<text text-anchor=\"middle\" x=\"715.73\" y=\"-49\" font-family=\"Arial\" font-size=\"20.00\">a2</text>\n",
       "</g>\n",
       "<!-- s2&#45;&gt;s2&#45;a2 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>s2&#45;&gt;s2&#45;a2</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M532.82,-54.13C539.49,-56.15 546.46,-57.91 553.15,-59 595.65,-65.9 645.23,-62.73 678.15,-59.09\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"678.7,-62.55 688.22,-57.9 677.88,-55.6 678.7,-62.55\"/>\n",
       "</g>\n",
       "<!-- s1&#45;a1&#45;&gt;s1 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>s1&#45;a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M691.43,-120.01C684.78,-116.72 677.38,-113.69 670.15,-112 619.51,-100.18 603.97,-100.96 553.15,-112 548.79,-112.95 544.36,-114.26 540,-115.8\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"538.65,-112.56 530.61,-119.46 541.2,-119.09 538.65,-112.56\"/>\n",
       "<text text-anchor=\"middle\" x=\"611.65\" y=\"-117.2\" font-family=\"Arial\" font-size=\"16.00\">p = 1 &#160;reward =1</text>\n",
       "</g>\n",
       "<!-- s2&#45;a2&#45;&gt;s2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>s2&#45;a2&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M690.2,-42.82C683.81,-40.42 676.83,-38.23 670.15,-37 628.37,-29.3 579.99,-30.97 544.87,-34.03\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"544.5,-30.55 534.87,-34.97 545.16,-37.52 544.5,-30.55\"/>\n",
       "<text text-anchor=\"middle\" x=\"611.65\" y=\"-42.2\" font-family=\"Arial\" font-size=\"16.00\">p = 1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f46585bcac0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vi_breaking_gamma = 0.95\n",
    "vi_breaking_mdp, _ = get_vi_breaking_mdp(gamma=0.95, N=99)\n",
    "display(plot_graph(vi_breaking_mdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_final, V_at_each_step = value_iteration(vi_breaking_mdp, vi_breaking_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  policy={'s0': 'a2', 's1': 'a1', 's2': 'a2'}\n",
      "  99  policy={'s0': 'a1', 's1': 'a1', 's2': 'a2'}\n"
     ]
    }
   ],
   "source": [
    "policy = {s: None for s in vi_breaking_mdp.get_all_states() }\n",
    "\n",
    "for i, V in enumerate(V_at_each_step):\n",
    "    new_policy = get_optimal_policy(vi_breaking_mdp, V, vi_breaking_gamma)\n",
    "\n",
    "    n_diff = len([\n",
    "        s for s in vi_breaking_mdp.get_all_states() \n",
    "        if new_policy[s] != policy[s]\n",
    "        ])\n",
    "    \n",
    "    if not are_policies_equal(new_policy, policy) > 0:\n",
    "        print(f'{i:4}  policy={new_policy}')\n",
    "    \n",
    "    policy = new_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration (PI) - 3 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Computes V^pi(s) FOR ALL STATES under given policy.\n",
    "    :param policy: a dict of currently chosen actions {s : a}\n",
    "    :returns: a dict {state : V^pi(state) for all states}\n",
    "    \"\"\"\n",
    "    all_states = mdp.get_all_states()\n",
    "    idx = {s: idx for idx, s in enumerate(sorted(all_states))}\n",
    "    idx2state = {idx: s for idx, s in enumerate(sorted(all_states))}\n",
    "    S = len(idx)\n",
    "    R = np.zeros(shape=S)\n",
    "    P = np.zeros(shape=(S, S))\n",
    "    for s in all_states:\n",
    "        if mdp.is_terminal(s):\n",
    "            continue\n",
    "        a = policy[s]\n",
    "        for next_s, p in mdp.get_next_states(s, a).items():  \n",
    "            P[idx[s]][idx[next_s]] = p\n",
    "            R[idx[s]] += p * mdp.get_reward(s, a, next_s)\n",
    "\n",
    "    R = R.reshape((S, 1))\n",
    "    V = np.linalg.inv(np.eye(S) - gamma*P) @ R\n",
    "    assert V.shape == (S, 1)\n",
    "\n",
    "    V = V.reshape(S)\n",
    "    return {idx2state[idx]: V[idx] for idx in range(S)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_action(mdp, s):\n",
    "    acts = mdp.get_possible_actions(s)\n",
    "    if len(acts) == 0:\n",
    "        return None\n",
    "    return np.random.choice(acts)\n",
    "\n",
    "\n",
    "def get_random_policy(mdp):\n",
    "    return {\n",
    "        s: get_random_action(mdp, s)\n",
    "        for s in mdp.get_all_states()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "        mdp, gamma, policy=None, \n",
    "        max_steps=1000\n",
    "        ):  \n",
    "    policy = policy or get_random_policy(mdp)   \n",
    "    all_policies_and_V = []\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        V = compute_V(mdp, policy, gamma)\n",
    "        all_policies_and_V.append((policy, V))\n",
    "\n",
    "        new_policy = get_optimal_policy(mdp, V, gamma)\n",
    "        if are_policies_equal(new_policy, policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, V, all_policies_and_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pi_mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.5)\n",
    "test_pi_gamma = 0.95\n",
    "policy, V, all_policies_and_V = policy_iteration(test_pi_mdp, test_pi_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(all_policies_and_V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение VI и PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi_breaking_mdp:\n",
      "  VI time = 0.02216921329498291+-0.004722466023039892\n",
      "  PI time = 0.0008510541915893555+-0.002280837571433603\n",
      "FrozenLake 4x4, slip_chance=0.2:\n",
      "  VI time = 0.01705634593963623+-0.00419952867966967\n",
      "  PI time = 0.004777874946594238+-0.0016434791719563443\n",
      "FrozenLake 8x8, slip_chance=0.2:\n",
      "  VI time = 0.11121118068695068+-0.015305721708707256\n",
      "  PI time = 0.04302286863327026+-0.00973593614075259\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    ('vi_breaking_mdp', vi_breaking_mdp, vi_breaking_gamma),\n",
    "    ('FrozenLake 4x4, slip_chance=0.2', \n",
    "        FrozenLakeEnv(map_name='4x4', slip_chance=0.2), 0.9),\n",
    "    ('FrozenLake 8x8, slip_chance=0.2', \n",
    "        FrozenLakeEnv(map_name='8x8', slip_chance=0.2), 0.9), \n",
    "]\n",
    "\n",
    "\n",
    "N = 100\n",
    "\n",
    "for name, mdp, gamma in test_cases:\n",
    "    optimal_policy, optimal_V, _ = policy_iteration(mdp, gamma)\n",
    "\n",
    "    times_vi = []\n",
    "    for _ in range(N):\n",
    "        t0 = time()\n",
    "        V, _ = value_iteration(mdp, gamma)\n",
    "        times_vi.append(time() - t0)\n",
    "        \n",
    "        policy = get_optimal_policy(mdp, V, gamma)\n",
    "        assert are_policies_equal(optimal_policy, policy)\n",
    "\n",
    "    times_pi = []\n",
    "    for _ in range(N):\n",
    "        t0 = time()\n",
    "        policy, V, _ = policy_iteration(mdp, gamma)\n",
    "        times_pi.append(time() - t0)\n",
    "\n",
    "        assert are_policies_equal(optimal_policy, policy)\n",
    "\n",
    "    print(f'{name}:')\n",
    "    print(f'  VI time = {np.mean(times_vi)}+-{np.std(times_vi)}')\n",
    "    print(f'  PI time = {np.mean(times_pi)}+-{np.std(times_pi)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
