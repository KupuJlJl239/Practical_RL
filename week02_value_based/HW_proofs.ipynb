{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration convervence proof (1 pts)\n",
    "**Note:** Assume that $\\mathcal{S}, \\mathcal{A}$ are finite.\n",
    "\n",
    "Update of value function in value iteration can be rewritten in a form of Bellman operator:\n",
    "\n",
    "$$(TV)(s) = \\max_{a \\in \\mathcal{A}}\\mathbb{E}\\left[ r_{t+1} + \\gamma V(s_{t+1}) | s_t = s, a_t = a\\right]$$\n",
    "\n",
    "Value iteration algorithm with Bellman operator:\n",
    "\n",
    "---\n",
    "&nbsp;&nbsp; Initialize $V_0$\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 0,1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $V_{k+1} \\leftarrow TV_k$\n",
    "\n",
    "&nbsp;&nbsp;**end for**\n",
    "\n",
    "---\n",
    "\n",
    "In [lecture](https://docs.google.com/presentation/d/1lz2oIUTvd2MHWKEQSH8hquS66oe4MZ_eRvVViZs2uuE/edit#slide=id.g4fd6bae29e_2_4) we established contraction property of bellman operator:\n",
    "\n",
    "$$\n",
    "||TV - TU||_{\\infty} \\le \\gamma ||V - U||_{\\infty}\n",
    "$$\n",
    "\n",
    "For all $V, U$\n",
    "\n",
    "Using contraction property of Bellman operator, Banach fixed-point theorem and Bellman equations prove that value function converges to $V^*$ in value iterateion$\n",
    "\n",
    "*Доказательство*\n",
    "\n",
    "$V^*$ - неподвижная точка оператора Беллмана $T$ (согласно уравнению Беллмана для оптимальной стратегии), причём $T$ - сжимающее отображение. По теореме о неподвижной точке сжимающего отображения $T^n(V) \\rightarrow V^*$ при $n \\rightarrow \\infty$, Ч.Т.Д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronious value iteration (2 pts)\n",
    "\n",
    "Consider the following algorithm:\n",
    "\n",
    "---\n",
    "\n",
    "Initialize $V_0$\n",
    "\n",
    "**for** $k = 0,1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Select some state $s_k \\in \\mathcal{S}$    \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $V(s_k) := (TV)(s_k)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Note that unlike common value iteration, here we update only a single state at a time.\n",
    "\n",
    "**Homework.** Prove the following proposition:\n",
    "\n",
    "If for all $s \\in \\mathcal{S}$, $s$ appears in the sequence $(s_0, s_1, ...)$ infinitely often, then $V$ converges to $V*$\n",
    "\n",
    "*Доказательство*\n",
    "\n",
    "Пусть $V_t$ - вектор полезностей всех состояний на шаге t, после этого шага изменится только одна его координата - $V(s_t)$. \n",
    "\n",
    "Оператор Беллмана сжимающий: $||TV - TU||_{\\infty} \\le \\gamma ||V - U||_{\\infty}$, \n",
    "поэтому $|V_{t+1}(s_t) - V^*(s_t)| \\le \\gamma ||V_t - V^*||$. \n",
    "\n",
    "Заметим, что $||V_{t+1} - V^*||_{\\infty} \\le ||V_t - V^*||_{\\infty}$. Это верно, так как векторы слева и справа под знаком нормы имеют все одинаковые координаты, кроме $s_t$, а для неё как мы поняли выше требуемое неравенство выполнено. \n",
    "\n",
    "Пусть $\\tau$ - номер итерации, для которого в наборе $(s_0, s_1, ... , s_{\\tau})$ каждое состояние встречается хотя бы один раз (такое $\\tau$ найдётся, так как каждое состояние встречается во всей последовательности бесконечное число раз). \n",
    "\n",
    "Рассмотрим любое состояние $s$. Пусть координата $V(s)$ в последний раз менялась в момент времени $t$.\n",
    "\n",
    "Тогда $|V_{\\tau}(s) - V^*(s)| = |V_{t+1}(s) - V^*(s)| \\le \\gamma ||V_t - V^*|| \\le \\gamma ||V_0 - V^*||$, а значит \n",
    "$||V_{\\tau} - V^*|| \\le \\gamma ||V_0 - V^*||$.\n",
    "\n",
    "Далее аналогично находим ${\\tau}_1$, для которого $||V_{{\\tau}_1} - V^*|| \\le \\gamma ||V_\\tau - V^*|| \\le \\gamma^2 ||V_0 - V^*||$, \n",
    "и таким образом продолжая дальше получаем, что $||V_t - V^*|| \\rightarrow 0$, что и требовалось.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration convergence (3 pts)\n",
    "\n",
    "**Note:** Assume that $\\mathcal{S}, \\mathcal{A}$ are finite.\n",
    "\n",
    "We can define another Bellman operator:\n",
    "\n",
    "$$(T_{\\pi}V)(s) = \\mathbb{E}_{r, s'|s, a = \\pi(s)}\\left[r + \\gamma V(s')\\right]$$\n",
    "\n",
    "And rewrite policy iteration algorithm in operator form:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Initialize $\\pi_0$\n",
    "\n",
    "**for** $k = 0,1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Solve $V_k = T_{\\pi_k}V_k$   \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Select $\\pi_{k+1}$ s.t. $T_{\\pi_{k+1}}V_k = TV_k$ \n",
    "\n",
    "**end for**\n",
    "\n",
    "---\n",
    "\n",
    "To prove convergence of the algorithm we need to prove two properties: contraction an monotonicity.\n",
    "\n",
    "#### Monotonicity (0.5 pts)\n",
    "\n",
    "For all $V, U$ if $V(s) \\le U(s)$   $\\forall s \\in \\mathcal{S}$ then $(T_\\pi V)(s) \\le (T_\\pi U)(s)$   $\\forall s \\in  \\mathcal{S}$\n",
    "\n",
    "*Доказательство*\n",
    "\n",
    "$(T_\\pi U)(s) - (T_\\pi V)(s) = \n",
    "\\mathbb{E}_{r, s'|s, a = \\pi(s)}(r + \\gamma U(s')) - \\mathbb{E}_{r, s'|s, a = \\pi(s)}(r + \\gamma V(s')) = \n",
    "\\mathbb{E}_{s'|s, a = \\pi(s)}\\gamma (U(s') - V(s')) \\ge 0$\n",
    "\n",
    "#### Contraction (1 pts)\n",
    "\n",
    "$$\n",
    "||T_\\pi V - T_\\pi U||_{\\infty} \\le \\gamma ||V - U||_{\\infty}\n",
    "$$\n",
    "\n",
    "For all $V, U$\n",
    "\n",
    "*Доказательство*\n",
    "\n",
    "Для любого состояния $s$: \n",
    "$$|T_\\pi V(s) - T_\\pi U(s)| = \n",
    "|\\gamma \\mathbb{E}_{s'|s, a = \\pi(s)}(U(s') - V(s'))| \\le\n",
    "\\gamma \\mathbb{E}_{s'|s, a = \\pi(s)}|U(s') - V(s')| \\le\n",
    "\\gamma \\max\\limits_{s'} |U(s') - V(s')| =\n",
    "\\gamma||V - U||_{\\infty}\n",
    "$$\n",
    "\n",
    "Поэтому и $||T_\\pi V - T_\\pi U||_{\\infty} = \\max\\limits_{s} |T_\\pi V(s) - T_\\pi U(s)| \\le \\gamma ||V - U||_{\\infty}$.\n",
    "\n",
    "#### Convergence (1.5 pts)\n",
    "\n",
    "Prove that there exists iteration $k_0$ such that $\\pi_k = \\pi^*$ for all $k \\ge k_0$\n",
    "\n",
    "\n",
    "*Доказательство*\n",
    "\n",
    "Введём отношение частичного порядка на векторах: будем говорить, что $V \\ge U$, если $\\forall s: V(s) \\ge U(s)$.\n",
    "\n",
    "Из определений операторов $T$ и $T_{\\pi_k}$ видно, что $\\forall V: TV \\ge T_{\\pi_k}V$, в частности $TV_k \\ge T_{\\pi_k}V_k = V_k$. \n",
    "Пользуясь этим фактом, докажем, что $V_{k+1} \\ge V_k$.\n",
    "\n",
    "Отображение $T_{\\pi_{k+1}}$ сжимающее и $V_{k+1}$ - его неподвижная точка, а потому $T_{\\pi_{k+1}}^n V_k \\to V_{k+1}$ при $n \\to \\infty$.\n",
    "Последовательность векторов $T_{\\pi_{k+1}}^n V_k$ нестрого возрастает. \n",
    "Действительно, $ T_{\\pi_{k+1}}^{n+1} V_k = T_{\\pi_{k+1}}^n (T V_k) \\ge T_{\\pi_{k+1}}^n V_k $, \n",
    "так как $T_{\\pi_{k+1}}$ (а значит и $T_{\\pi_{k+1}}^n$) монотонно.\n",
    "\n",
    "Значит $V_k = T_{\\pi_{k+1}}^{0} V_k \\le T_{\\pi_{k+1}}^{1} V_k \\le ... \\le V_{k+1}$.\n",
    "\n",
    "\n",
    "Таким образом, последовательность политик $\\pi_k$ также неубывает. Заметим, что все эти политики - детерминированные, и поскольку множества состояний и действий конечны, множество детерминированных политик тоже. Раз так, последовательность $\\pi_k$ стабилизируется на некоторой политике $\\pi$ начиная с какого-то момента, полезности состояний также стабилизируются на $V$.\n",
    "\n",
    "\n",
    "Из равенства $V_{k+1} = V_k = V$ следует, что\n",
    " $V = T_{\\pi_{k+1}}^{0} V = T_{\\pi_{k+1}}^{1} V = ... = V$, \n",
    " а значит $T V = V$.\n",
    "\n",
    " На лекции мы доказывали, что в таком случае политика $\\pi$ является оптимальной.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
